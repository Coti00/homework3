{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 채용 정보 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 기본 URL 및 페이지 범위 설정\n",
    "base_url = \"https://www.saramin.co.kr/zf_user/jobs/list/job-category\"\n",
    "page_range = range(1, 41)  # 1페이지부터 40페이지까지\n",
    "\n",
    "# User-Agent 설정\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
    "}\n",
    "\n",
    "# 데이터 저장할 리스트\n",
    "data = []\n",
    "rec_idx_set = set()  # rec_idx를 저장할 집합 (중복 제거를 위해)\n",
    "\n",
    "# 각 페이지에 대해 요청 수행\n",
    "for page in page_range:\n",
    "    # 각 페이지에 대한 URL 생성\n",
    "    url = f\"{base_url}?page={page}&cat_kewd=2248%2C82%2C83%2C84%2C87&search_optional_item=n&search_done=y&panel_count=y&preview=y&isAjaxRequest=0&page_count=50&sort=RL&type=job-category&is_param=1&isSearchResultEmpty=1&isSectionHome=0&searchParamCount=1#searchTitle\"\n",
    "\n",
    "    # HTTP GET 요청\n",
    "    response = requests.get(url, headers=headers, allow_redirects=True)\n",
    "\n",
    "    # HTTP 응답 상태 코드 확인\n",
    "    if response.status_code == 200:\n",
    "        # BeautifulSoup으로 HTML 파싱\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 구인 정보가 담긴 div 선택\n",
    "        job_list = soup.find_all('div', class_='box_item')\n",
    "\n",
    "        # 구인 정보 추출\n",
    "        for job in job_list:\n",
    "            company = job.find('a', class_='str_tit')\n",
    "            title = job.find('div', class_='job_tit').find('a')\n",
    "            location = job.find('p', class_='work_place')\n",
    "            career = job.find('p', class_='career').get_text(strip=True) if job.find('p', class_='career') else '정보 없음'\n",
    "            education = job.find('p', class_='education').get_text(strip=True) if job.find('p', class_='education') else '정보 없음'\n",
    "            job_sector = job.find('span', class_='job_sector')\n",
    "\n",
    "            # job_sector 정보를 리스트로 저장\n",
    "            job_sector_list = [sector.get_text(strip=True) for sector in job_sector.find_all('span')] if job_sector else []\n",
    "\n",
    "            # career를 리스트로 변환\n",
    "            career_list = [c.strip() for c in career.split('·')]  # '·'를 기준으로 분리\n",
    "\n",
    "            # education에서 '↑' 기호를 제거\n",
    "            education = education.replace('↑', '')\n",
    "\n",
    "            # rec_idx 추출\n",
    "            rec_idx = title['href'].split('rec_idx=')[-1].split('&')[0]  # rec_idx 값을 추출\n",
    "            rec_idx_set.add(rec_idx)  # rec_idx를 집합에 추가하여 중복 제거\n",
    "\n",
    "            # 데이터 딕셔너리 생성\n",
    "            job_data = {\n",
    "                \"company\": company.get_text(strip=True) if company else '정보 없음',\n",
    "                \"title\": title.get_text(strip=True) if title else '정보 없음',\n",
    "                \"link\": 'https://www.saramin.co.kr' + title['href'] if title else '정보 없음',\n",
    "                \"location\": location.get_text(strip=True) if location else '정보 없음',\n",
    "                \"career\": career_list,\n",
    "                \"education\": education.strip() if education else '정보 없음',\n",
    "                \"job_sector\": job_sector_list,\n",
    "                \"view\": 0  # view 필드를 추가하고 기본값 0으로 설정\n",
    "            }\n",
    "\n",
    "            data.append(job_data)\n",
    "\n",
    "            # 일정한 간격 두기 (1초에서 3초 사이의 랜덤 값)\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from page {page}: {response.status_code}\")\n",
    "\n",
    "# JSON 파일로 채용 공고 데이터 저장\n",
    "with open('job_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"데이터가 job_data.json 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기업정보 아이디"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd  # pandas를 추가합니다.\n",
    "\n",
    "# 기본 URL 및 페이지 범위 설정\n",
    "base_url = \"https://www.saramin.co.kr/zf_user/jobs/list/job-category\"\n",
    "page_range = range(1, 41)  # 1페이지부터 40페이지까지\n",
    "\n",
    "# User-Agent 설정\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
    "}\n",
    "\n",
    "# 데이터 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 각 페이지에 대해 요청 수행\n",
    "for page in page_range:\n",
    "    # 각 페이지에 대한 URL 생성\n",
    "    url = f\"{base_url}?page={page}&cat_kewd=2248%2C82%2C83%2C84%2C87&search_optional_item=n&search_done=y&panel_count=y&preview=y&isAjaxRequest=0&page_count=20&sort=RL&type=job-category&is_param=1&isSearchResultEmpty=1&isSectionHome=0&searchParamCount=1#searchTitle\"\n",
    "\n",
    "    # HTTP GET 요청\n",
    "    response = requests.get(url, headers=headers, allow_redirects=True)\n",
    "\n",
    "    # HTTP 응답 상태 코드 확인\n",
    "    if response.status_code == 200:\n",
    "        # BeautifulSoup으로 HTML 파싱\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 구인 정보가 담긴 div 선택\n",
    "        job_list = soup.find_all('div', class_='box_item')\n",
    "\n",
    "        # 구인 정보 추출\n",
    "        for job in job_list:\n",
    "            company = job.find('a', class_='str_tit')\n",
    "\n",
    "            # 데이터 딕셔너리 생성\n",
    "            job_data = company['href'].split('csn=')[-1] if company and company['href'] else '정보 없음'  # csn 값만 저장\n",
    "\n",
    "            data.append(job_data)\n",
    "\n",
    "            # 일정한 간격 두기 (1초에서 3초 사이의 랜덤 값)\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from page {page}: {response.status_code}\")\n",
    "\n",
    "data = list(set(data))\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('company_id.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"데이터가 JSON 파일과 CSV 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기업정보가 등록되지 않은 요소는 삭제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path = 'company_id.json'\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)  # JSON 데이터 파싱하여 data 리스트에 저장\n",
    "\n",
    "# '/'로 시작하는 요소를 제거\n",
    "data = [item for item in data if not item.startswith('/')]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"데이터에서 '/'로 시작하는 요소가 삭제되었습니다.\")\n",
    "print(data)  # 최종 필터링된 결과 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 정보만 가져오도록 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re  # 정규 표현식 사용을 위해 import\n",
    "\n",
    "# User-Agent 설정\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
    "}\n",
    "\n",
    "# 데이터 저장할 리스트\n",
    "data_list = []\n",
    "\n",
    "# 각 페이지에 대해 요청 수행\n",
    "for link in data:  # data 리스트에 csn 링크가 들어있다고 가정\n",
    "    # 각 페이지에 대한 URL 생성\n",
    "    url = f\"https://www.saramin.co.kr/zf_user/company-info/view?csn={link}\"\n",
    "    \n",
    "    # HTTP GET 요청\n",
    "    response = requests.get(url, headers=headers, allow_redirects=True)\n",
    "\n",
    "    # HTTP 응답 상태 코드 확인\n",
    "    if response.status_code == 200:\n",
    "        # BeautifulSoup으로 HTML 파싱\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 회사 정보 추출\n",
    "        company_info = {\n",
    "            \"company_name\": \"정보 없음\",\n",
    "            \"company_type\": \"정보 없음\",\n",
    "            \"employee_count\": 0,\n",
    "            \"industry\": \"정보 없음\",\n",
    "            \"ceo_name\": \"정보 없음\",\n",
    "            \"website\": \"정보 없음\",\n",
    "            \"description\": \"정보 없음\",\n",
    "            \"address\": \"정보 없음\",\n",
    "            \"established_date\": \"정보 없음\",\n",
    "            \"revenue\": \"정보 없음\",  # revenue 초기값을 \"정보 없음\"으로 설정\n",
    "            \"salary\": \"정보 없음\"  # salary 초기값을 \"정보 없음\"으로 설정\n",
    "        }\n",
    "\n",
    "        # 회사명 추출 및 \"기업정보\" 제거\n",
    "        company_name_elem = soup.find('h1', class_='tit_company')\n",
    "        if company_name_elem:\n",
    "            company_info['company_name'] = company_name_elem.get_text(strip=True).replace('기업정보', '').strip()\n",
    "\n",
    "        # 업력, 기업형태, 사원수 추출\n",
    "        company_summary_items = soup.find_all('li', class_='company_summary_item')\n",
    "        for item in company_summary_items:\n",
    "            title = item.find('strong', class_='company_summary_tit').get_text(strip=True)\n",
    "            description = item.find('p', class_='company_summary_desc').get_text(strip=True)\n",
    "\n",
    "            # 설립일 정보에서 '설립' 단어 제거\n",
    "            if '설립' in description:\n",
    "                establishment_date = re.sub(\"설립\", \"\", description).strip()  # '설립' 단어 제거\n",
    "                company_info['established_date'] = establishment_date\n",
    "\n",
    "            # 사원수 정보에서 숫자만 추출하고 '명' 제거\n",
    "            if '사원수' in description:\n",
    "                number = re.findall(r'\\d+', title)  # 숫자 추출\n",
    "                company_info['employee_count'] = int(number[0]) if number else 0  # '명' 제거 및 숫자형으로 변환\n",
    "\n",
    "            # 기업형태 정보\n",
    "            elif '기업형태' in description:\n",
    "                company_info['company_type'] = title if title else '정보 없음'\n",
    "\n",
    "        # 매출액 추출\n",
    "        revenue_elem = soup.find('strong', class_='company_summary_tit', text=re.compile(r'억|만원'))\n",
    "        if revenue_elem:\n",
    "            company_info['revenue'] = revenue_elem.get_text(strip=True)  # 매출액 정보 그대로 저장\n",
    "        else:\n",
    "            company_info['revenue'] = \"정보 없음\"  # 매출액 정보가 없으면 \"정보 없음\"으로 설정\n",
    "\n",
    "        # 연봉 정보 추출\n",
    "        salary_url = f\"https://www.saramin.co.kr/zf_user/company-info/view-inner-salary?csn={link}\"\n",
    "        salary_response = requests.get(salary_url, headers=headers)  # 연봉 정보를 위한 추가 요청\n",
    "        if salary_response.status_code == 200:\n",
    "            salary_soup = BeautifulSoup(salary_response.text, 'html.parser')\n",
    "            salary_elem = salary_soup.find('p', class_='average_currency')\n",
    "            if salary_elem:\n",
    "                salary_value = salary_elem.find('em')\n",
    "                if salary_value:\n",
    "                    company_info['salary'] = salary_value.get_text(strip=True)  # 연봉 정보를 그대로 저장\n",
    "                else:\n",
    "                    company_info['salary'] = \"정보 없음\"  # 연봉 정보가 없으면 \"정보 없음\"으로 설정\n",
    "            else:\n",
    "                company_info['salary'] = \"정보 없음\"  # 연봉 정보 요청이 성공했으나 요소가 없을 경우\n",
    "        else:\n",
    "            company_info['salary'] = \"정보 없음\"  # 연봉 정보 요청 실패 시 \"정보 없음\"\n",
    "\n",
    "        # 업종, 대표자명, 홈페이지, 사업내용, 주소 추출\n",
    "        details = soup.find_all('div', class_='company_details_group')\n",
    "        for detail in details:\n",
    "            title = detail.find('dt', class_='tit').get_text(strip=True)\n",
    "            description = detail.find('dd', class_='desc').get_text(strip=True)\n",
    "            if title == '업종':\n",
    "                company_info['industry'] = description.strip()  # 업종 정보 저장\n",
    "            elif title == '대표자명':\n",
    "                company_info['ceo_name'] = description.strip()  # CEO 이름 저장\n",
    "            elif title == '홈페이지':\n",
    "                company_info['website'] = description.strip()  # 웹사이트 저장\n",
    "            elif title == '사업내용':\n",
    "                company_info['description'] = description.strip()  # 설명 저장\n",
    "            elif title == '주소':\n",
    "                company_info['address'] = description.strip()  # 주소 저장\n",
    "\n",
    "        # 회사 정보 딕셔너리를 리스트에 추가\n",
    "        data_list.append(company_info)\n",
    "\n",
    "        # 일정한 간격 두기 (1초에서 3초 사이의 랜덤 값)\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url}: {response.status_code}\")\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('company_info.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"데이터가 JSON 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path = 'company_info.json'\n",
    "\n",
    "# 문자열 형태의 revenue를 숫자로 변환하는 함수\n",
    "def convert_revenue_to_number(revenue):\n",
    "    if revenue == \"정보 없음\":\n",
    "        return 0  # \"정보 없음\"일 경우 0으로 설정\n",
    "\n",
    "    total = 0\n",
    "    # 공백을 기준으로 분리\n",
    "    parts = revenue.split(\" \")\n",
    "\n",
    "    # 각 요소에 대해 처리\n",
    "    for part in parts:\n",
    "        # 숫자와 단위를 분리\n",
    "        num_str = part[:-1]  # 마지막 문자(단위)를 제외한 부분\n",
    "        unit = part[-1]  # 기본적으로 마지막 문자(단위)\n",
    "\n",
    "        # 마지막 두 문자가 \"만원\"인 경우 처리\n",
    "        if len(part) > 2 and part[-2:] == \"만원\":\n",
    "            unit = \"만원\"  # 단위를 \"만원\"으로 설정\n",
    "            num_str = part[:-2]  # \"만원\" 부분 제거\n",
    "        else:\n",
    "            unit = part[-1]  # 단위가 \"억\"이나 \"조\"일 경우\n",
    "\n",
    "        # 숫자에서 ','를 제거\n",
    "        num_str = num_str.replace(\",\", \"\")\n",
    "\n",
    "        # 각 단위에 따라 계산\n",
    "        if \"조\" in unit:\n",
    "            total += int(num_str) * 1_000_000_000_000  # 조 단위\n",
    "        elif \"억\" in unit:\n",
    "            total += int(num_str) * 1_000_000_000  # 억 단위\n",
    "        elif \"만원\" in unit:\n",
    "            total += int(num_str) * 10_000  # 만원 단위\n",
    "\n",
    "    return total\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)  # JSON 데이터 파싱\n",
    "\n",
    "# revenue 필드 변환\n",
    "for company in data:\n",
    "    # 기존 revenue 값을 revenue2로 저장\n",
    "    company['revenue2'] = company['revenue']\n",
    "    # revenue 값을 숫자로 변환\n",
    "    company['revenue'] = convert_revenue_to_number(company['revenue'])  # revenue 변환\n",
    "\n",
    "# 수정된 데이터를 JSON 파일에 저장\n",
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"revenue 필드가 변환되고 기존 값은 revenue2로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "job_data_file = 'job_data.json'\n",
    "company_info_file = 'company_info.json'\n",
    "\n",
    "# job_data.json 파일 읽기\n",
    "with open(job_data_file, 'r', encoding='utf-8') as f:\n",
    "    job_data = json.load(f)  # JSON 데이터 파싱\n",
    "\n",
    "# company_info.json 파일 읽기\n",
    "with open(company_info_file, 'r', encoding='utf-8') as f:\n",
    "    company_info_data = json.load(f)  # JSON 데이터 파싱\n",
    "\n",
    "# 회사명을 키로 급여 정보를 매핑\n",
    "salary_mapping = {company['company_name']: company['salary'] for company in company_info_data}\n",
    "\n",
    "# 급여 정보를 숫자형으로 변환하는 함수\n",
    "def convert_salary_to_number(salary):\n",
    "    if salary == \"정보 없음\":\n",
    "        return 0  # 급여 정보가 없으면 0으로 설정\n",
    "    try:\n",
    "        # 급여 문자열에서 숫자만 추출하고 변환\n",
    "        salary_number = int(salary.replace(\",\", \"\").replace(\"만원\", \"\").strip()) * 10000\n",
    "        return salary_number\n",
    "    except ValueError:\n",
    "        return 0  # 변환이 실패할 경우 0으로 설정\n",
    "\n",
    "# job_data에 급여 정보 추가\n",
    "for job in job_data:\n",
    "    company_name = job.get('company')\n",
    "    salary_info = salary_mapping.get(company_name, \"정보 없음\")  # 급여 정보 가져오기\n",
    "    job['salary'] = convert_salary_to_number(salary_info)  # 숫자형으로 변환하여 추가\n",
    "\n",
    "# 수정된 데이터를 job_data.json 파일에 저장\n",
    "with open(job_data_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(job_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"급여 정보가 숫자형으로 job_data.json에 추가되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
